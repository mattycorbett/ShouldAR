# Copyright 2019 The MediaPipe Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Copied from mediapipe/graphs/iris_tracking/iris_tracking_gpu.pbtxt
#
# CHANGES:
#   - `input_video` is ImageFrame (ImageFrameToGpuBufferCalculator converts it into GpuBuffer)
#   - Add ImageTransformationCalculator and rotate the input
#   - Remove IrisAndDepthRendererGpu
#   - Add PacketPresenceCalculators

# MediaPipe graph that performs iris tracking with TensorFlow Lite on GPU.
# Used in the examples in
# mediapipie/examples/android/src/java/com/mediapipe/apps/iristrackinggpu and

# Input image. (ImageFrame)
input_stream: "input_video"

output_stream: "face_landmarks_with_iris"
output_stream: "face_rect_first"
output_stream: "face_detections"
output_stream: "multi_face_landmarks"

# Throttles the images flowing downstream for flow control. It passes through
# the very first incoming image unaltered, and waits for downstream nodes
# (calculators and subgraphs) in the graph to finish their tasks before it
# passes through another image. All images that come in while waiting are
# dropped, limiting the number of in-flight images in most part of the graph to
# 1. This prevents the downstream nodes from queuing up incoming images and data
# excessively, which leads to increased latency and memory usage, unwanted in
# real-time mobile applications. It also eliminates unnecessarily computation,
# e.g., the output produced by a node may get dropped downstream if the
# subsequent nodes are still busy processing previous inputs.
node {
  calculator: "FlowLimiterCalculator"
  input_stream: "input_video"
  input_stream: "FINISHED:face_landmarks_with_iris"
  input_stream_info: {
    tag_index: "FINISHED"
    back_edge: true
  }
  output_stream: "throttled_input_video"
}

node: {
  calculator: "ImageFrameToGpuBufferCalculator"
  input_stream: "throttled_input_video"
  output_stream: "throttled_input_video_gpu"
}

node: {
  calculator: "ImageTransformationCalculator"
  input_stream: "IMAGE_GPU:throttled_input_video_gpu"
  input_side_packet: "ROTATION_DEGREES:input_rotation"
  input_side_packet: "FLIP_HORIZONTALLY:input_horizontally_flipped"
  input_side_packet: "FLIP_VERTICALLY:input_vertically_flipped"
  output_stream: "IMAGE_GPU:transformed_input_video"
}

# Defines how many faces to detect. Iris tracking currently only handles one
# face (left and right eye), and therefore this should always be set to 1.
node {
  calculator: "ConstantSidePacketCalculator"
  output_side_packet: "PACKET:num_faces"
  node_options: {
    [type.googleapis.com/mediapipe.ConstantSidePacketCalculatorOptions]: {
      packet { int_value: 1 }
    }
  }
}

# Caches face rects calculated from landmarks, and upon the arrival of the next
# input image, sends out the cached rects with timestamps replaced by that of
# the input image, essentially generating a packet that carries the previous
# face rects. Note that upon the arrival of the very first input image, a
# timestamp bound update occurs to jump start the feedback loop.
node {
  calculator: "PreviousLoopbackCalculator"
  input_stream: "MAIN:transformed_input_video"
  input_stream: "LOOP:face_rects_from_landmarks"
  input_stream_info: {
    tag_index: "LOOP"
    back_edge: true
  }
  output_stream: "PREV_LOOP:prev_face_rects_from_landmarks"
}

# When the optional input side packet "use_prev_landmarks" is either absent or
# set to true, uses the landmarks on the previous image to help localize
# landmarks on the current image.
node {
  calculator: "GateCalculator"
  input_side_packet: "ALLOW:use_prev_landmarks"
  input_stream: "prev_face_rects_from_landmarks"
  output_stream: "gated_prev_face_rects_from_landmarks"
  options: {
    [mediapipe.GateCalculatorOptions.ext] {
      allow: true
    }
  }
}

# Determines if an input vector of NormalizedRect has a size greater than or
# equal to the provided num_faces.
node {
  calculator: "NormalizedRectVectorHasMinSizeCalculator"
  input_stream: "ITERABLE:gated_prev_face_rects_from_landmarks"
  input_side_packet: "num_faces"
  output_stream: "prev_has_enough_faces"
}

# Drops the incoming image if enough faces have already been identified from the
# previous image. Otherwise, passes the incoming image through to trigger a new
# round of face detection.
node {
  calculator: "GateCalculator"
  input_stream: "transformed_input_video"
  input_stream: "DISALLOW:prev_has_enough_faces"
  output_stream: "gated_image"
  options: {
    [mediapipe.GateCalculatorOptions.ext] {
      empty_packets_as_allow: true
    }
  }
}

# Detects faces.
node {
  calculator: "FaceDetectionFullRangeGpu"
  input_stream: "IMAGE:gated_image"
  output_stream: "DETECTIONS:all_face_detections"
}

# Makes sure there are no more detections than the provided num_faces.
node {
  calculator: "ClipDetectionVectorSizeCalculator"
  input_stream: "all_face_detections"
  output_stream: "face_detections"
  input_side_packet: "num_faces"
}

# Calculate size of the image.
node {
  calculator: "ImagePropertiesCalculator"
  input_stream: "IMAGE_GPU:gated_image"
  output_stream: "SIZE:gated_image_size"
}

# Outputs each element of face_detections at a fake timestamp for the rest of
# the graph to process. Clones the image size packet for each face_detection at
# the fake timestamp. At the end of the loop, outputs the BATCH_END timestamp
# for downstream calculators to inform them that all elements in the vector have
# been processed.
node {
  calculator: "BeginLoopDetectionCalculator"
  input_stream: "ITERABLE:face_detections"
  input_stream: "CLONE:gated_image_size"
  output_stream: "ITEM:face_detection"
  output_stream: "CLONE:detections_loop_image_size"
  output_stream: "BATCH_END:detections_loop_end_timestamp"
}

# Calculates region of interest based on face detections, so that can be used
# to detect landmarks.
node {
  calculator: "FaceDetectionFrontDetectionToRoi"
  input_stream: "DETECTION:face_detection"
  input_stream: "IMAGE_SIZE:detections_loop_image_size"
  output_stream: "ROI:face_rect_from_detection"
}

# Collects a NormalizedRect for each face into a vector. Upon receiving the
# BATCH_END timestamp, outputs the vector of NormalizedRect at the BATCH_END
# timestamp.
node {
  calculator: "EndLoopNormalizedRectCalculator"
  input_stream: "ITEM:face_rect_from_detection"
  input_stream: "BATCH_END:detections_loop_end_timestamp"
  output_stream: "ITERABLE:face_rects_from_detections"
}

# Performs association between NormalizedRect vector elements from previous
# image and rects based on face detections from the current image. This
# calculator ensures that the output face_rects vector doesn't contain
# overlapping regions based on the specified min_similarity_threshold.
node {
  calculator: "AssociationNormRectCalculator"
  input_stream: "face_rects_from_detections"
  input_stream: "gated_prev_face_rects_from_landmarks"
  output_stream: "face_rects"
  options: {
    [mediapipe.AssociationCalculatorOptions.ext] {
      min_similarity_threshold: 0.5
    }
  }
}

# Calculate size of the image.
node {
  calculator: "ImagePropertiesCalculator"
  input_stream: "IMAGE_GPU:transformed_input_video"
  output_stream: "SIZE:image_size"
}

# Outputs each element of face_rects at a fake timestamp for the rest of the
# graph to process. Clones image and image size packets for each
# single_face_rect at the fake timestamp. At the end of the loop, outputs the
# BATCH_END timestamp for downstream calculators to inform them that all
# elements in the vector have been processed.
node {
  calculator: "BeginLoopNormalizedRectCalculator"
  input_stream: "ITERABLE:face_rects"
  input_stream: "CLONE:0:transformed_input_video"
  input_stream: "CLONE:1:image_size"
  output_stream: "ITEM:face_rect"
  output_stream: "CLONE:0:landmarks_loop_image"
  output_stream: "CLONE:1:landmarks_loop_image_size"
  output_stream: "BATCH_END:landmarks_loop_end_timestamp"
}

# Detects face landmarks within specified region of interest of the image.
node {
  calculator: "FaceLandmarkGpu"
  input_stream: "IMAGE:landmarks_loop_image"
  input_stream: "ROI:face_rect"
  output_stream: "LANDMARKS:face_landmarks"
}

# Calculates region of interest based on face landmarks, so that can be reused
# for subsequent image.
node {
  calculator: "FaceLandmarkLandmarksToRoi"
  input_stream: "LANDMARKS:face_landmarks"
  input_stream: "IMAGE_SIZE:landmarks_loop_image_size"
  output_stream: "ROI:face_rect_from_landmarks"
}

# Collects a set of landmarks for each face into a vector. Upon receiving the
# BATCH_END timestamp, outputs the vector of landmarks at the BATCH_END
# timestamp.
node {
  calculator: "EndLoopNormalizedLandmarkListVectorCalculator"
  input_stream: "ITEM:face_landmarks"
  input_stream: "BATCH_END:landmarks_loop_end_timestamp"
  output_stream: "ITERABLE:multi_face_landmarks"
}

# Collects a NormalizedRect for each face into a vector. Upon receiving the
# BATCH_END timestamp, outputs the vector of NormalizedRect at the BATCH_END
# timestamp.
node {
  calculator: "EndLoopNormalizedRectCalculator"
  input_stream: "ITEM:face_rect_from_landmarks"
  input_stream: "BATCH_END:landmarks_loop_end_timestamp"
  output_stream: "ITERABLE:face_rects_from_landmarks"
}

# Detects faces and corresponding landmarks.
#node {
#  calculator: "FaceLandmarkFrontGpu"
#  input_stream: "IMAGE:transformed_input_video"
#  input_side_packet: "NUM_FACES:num_faces"
#  output_stream: "LANDMARKS:multi_face_landmarks"
 # output_stream: "ROIS_FROM_LANDMARKS:face_rects_from_landmarks"
#  output_stream: "DETECTIONS:face_detections"
#  output_stream: "ROIS_FROM_DETECTIONS:face_rects_from_detections"
#}

# Gets the very first and only face from "multi_face_landmarks" vector.
node {
  calculator: "SplitNormalizedLandmarkListVectorCalculator"
  input_stream: "multi_face_landmarks"
  output_stream: "face_landmarks_first"
  node_options: {
    [type.googleapis.com/mediapipe.SplitVectorCalculatorOptions] {
      ranges: { begin: 0 end: 1 }
      element_only: true
    }
  }
}

# Gets the very first and only face rect from "face_rects_from_landmarks"
# vector.
node {
  calculator: "SplitNormalizedRectVectorCalculator"
  input_stream: "face_rects_from_landmarks"
  output_stream: "face_rect_first"
  node_options: {
    [type.googleapis.com/mediapipe.SplitVectorCalculatorOptions] {
      ranges: { begin: 0 end: 1 }
      element_only: true
    }
  }
}

# Detects iris landmarks
node {
  calculator: "IrisLandmarksFromFaceLandmarksGpu"
  input_stream: "IMAGE:transformed_input_video"
  input_stream: "FACE_LANDMARKS:face_landmarks_first"
  output_stream: "UPDATED_FACE_LANDMARKS:updated_face_landmarks"
  output_stream: "LEFT_EYE_IRIS_LANDMARKS:left_iris_landmarks"
  output_stream: "RIGHT_EYE_IRIS_LANDMARKS:right_iris_landmarks"
}

# Concatenate iris landmarks from both eyes.
node {
  calculator: "ConcatenateNormalizedLandmarkListCalculator"
  input_stream: "updated_face_landmarks"
  input_stream: "left_iris_landmarks"
  input_stream: "right_iris_landmarks"
  output_stream: "face_landmarks_with_iris"
}
